"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn

"""
Activation functions
"""
class SiLU(nn.Module):
    @staticmethod
    def forward(x):
        ...
    


class Hardswish(nn.Module):
    @staticmethod
    def forward(x):
        ...
    


class Mish(nn.Module):
    @staticmethod
    def forward(x):
        ...
    


class MemoryEfficientMish(nn.Module):
    class F(torch.autograd.Function):
        @staticmethod
        def forward(ctx, x):
            ...
        
        @staticmethod
        def backward(ctx, grad_output):
            ...
        
    
    
    def forward(self, x):
        ...
    


class FReLU(nn.Module):
    def __init__(self, c1, k=...) -> None:
        ...
    
    def forward(self, x): # -> Tensor:
        ...
    


class AconC(nn.Module):
    r""" ACON activation (activate or not)
    AconC: (p1*x-p2*x) * sigmoid(beta*(p1*x-p2*x)) + p2*x, beta is a learnable parameter
    according to "Activate or Not: Learning Customized Activation" <https://arxiv.org/pdf/2009.04759.pdf>.
    """
    def __init__(self, c1) -> None:
        ...
    
    def forward(self, x):
        ...
    


class MetaAconC(nn.Module):
    r""" ACON activation (activate or not)
    MetaAconC: (p1*x-p2*x) * sigmoid(beta*(p1*x-p2*x)) + p2*x, beta is generated by a small network
    according to "Activate or Not: Learning Customized Activation" <https://arxiv.org/pdf/2009.04759.pdf>.
    """
    def __init__(self, c1, k=..., s=..., r=...) -> None:
        ...
    
    def forward(self, x):
        ...
    


